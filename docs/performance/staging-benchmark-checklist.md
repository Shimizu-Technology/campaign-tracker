# Staging Benchmark Checklist (11.3 Sign-off)

Use this checklist to run a clean, repeatable benchmark pass in staging and decide whether `11.3` can be marked `done`.

## 1) Preconditions

- Run against staging (or isolated benchmark environment), not production.
- Confirm no heavy background jobs/deploys are running during the measurement window.
- Use stable settings for comparability:
  - `DATASET_SIZE=30000`
  - `REPORTS_PER_PRECINCT=8`
  - `ITERATIONS=10`
  - `SEED=20260213`

## 2) One-command run

From repo root:

`api/bin/performance_staging_run`

Or explicit env values:

`DATASET_SIZE=30000 REPORTS_PER_PRECINCT=8 ITERATIONS=10 SEED=20260213 api/bin/performance_staging_run`

## 3) Collect artifacts

- `docs/performance/baseline-latest.json`
- timestamped JSON generated by the run (`docs/performance/baseline-*.json`)
- Optional: screenshot of runtime environment metrics (CPU/memory)

## 4) Sign-off checks

Pass criteria for this run:

- Query-shape guardrails:
  - `war_room_index_payload.sql_queries.avg <= 7`
  - `poll_watcher_index_payload.sql_queries.avg <= 3`
  - `supporters_index_filtered_search.sql_queries.avg <= 4`
- No endpoint shows SQL query explosion relative to current optimized baseline.
- p95 timings are directionally consistent with optimized baseline (allowing staging variance).
- Manual UI checks completed:
  - `/admin/supporters` search + sort
  - `/admin` dashboard load
  - `/admin/war-room` load + refresh
  - `/admin/poll-watcher` filter interaction

## 5) Record results in tracker

Update:

- `docs/performance/baseline-report-2026-02-13.md` with staging run summary
- `docs/execution-tracker.md` under item `11.3` with:
  - run timestamp
  - artifact paths
  - pass/fail decision

If all checks pass, mark `11.3` as `done`.
